{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "sample_subjects_train = pd.read_csv(\"data/sample_subjects_train.csv\")\n",
    "\n",
    "with open('data/sample_vectors_train.pkl', 'rb') as f:\n",
    "    train_vectors = pickle.load(f)\n",
    "\n",
    "sample_subjects_test = pd.read_csv(\"data/sample_subjects_test.csv\")\n",
    "\n",
    "with open('data/sample_vectors_test.pkl', 'rb') as f:\n",
    "    test_vectors = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.manifold import TSNE\n",
    "# import plotly.express as px\n",
    "\n",
    "# features = train_vectors\n",
    "\n",
    "# tsne = TSNE(n_components=2, random_state=0)\n",
    "# projections = tsne.fit_transform(features, )\n",
    "\n",
    "# fig = px.scatter(\n",
    "#     projections, x=0, y=1,\n",
    "#     color=sample_subjects_train['subject'], labels={'color': 'subjects'}\n",
    "# )\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'subject', 'exercise_type', 'difficulty', 'grade_level',\n",
       "       'modifier', 'content'],\n",
       "      dtype='object')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(sample_subjects_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column = 'subject'\n",
    "column = 'grade_level'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate centroids (mean vectors) for each subject\n",
    "centroids = sample_subjects_train.groupby(column).apply(lambda group: train_vectors[group.index].mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Predictions: ['G', '2', 'G', 'G', 'G', 'G', 'G', '3', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', '1', 'G', '1', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', '5', '5', '5', '3', '5', '2', '2', '1', '5', '5', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', '3', '5', '4', '5', '3', '5', '6', '4', '4', '3', '5', '1', '1', '1', '1', 'G', '3', '1', '1', '1', '6', '4', '6', 'G', '4', '5', '6', '2', '4', '6', '3', '3', '2', '6', '4', '5', '2', '5', '1', '6', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G']\n",
       "True Labels: ['G' 'G' 'G' 'G' 'G' 'G' 'G' 'G' 'G' 'G' '3' '6' '4' '1' '6' '6' '3' '3'\n",
       " '1' '1' 'G' 'G' 'G' 'G' 'G' 'G' 'G' 'G' 'G' 'G' '3' '2' '2' '5' '3' '3'\n",
       " '2' '1' '3' '5' 'G' 'G' 'G' 'G' 'G' 'G' 'G' 'G' 'G' 'G' '4' '1' '6' '3'\n",
       " '3' '6' '6' '4' '3' '6' '2' '3' '1' '2' '1' '2' '5' '6' '2' '3' '6' '4'\n",
       " '6' '6' '5' '5' '6' '1' '4' '5' '4' '5' '1' '4' '4' '4' '1' '5' '2' '3'\n",
       " 'G' 'G' 'G' 'G' 'G' 'G' 'G' 'G' 'G' 'G']\n",
       "Accuracy: 0.55\n",
       "Precision: 0.40294229579943863\n",
       "Recall: 0.3863858363858364\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "# Function to find the closest subject centroid\n",
    "def find_closest_subject(test_vector, centroids):\n",
    "    distances = centroids.apply(lambda centroid: euclidean(centroid, test_vector))\n",
    "    return distances.idxmin()\n",
    "\n",
    "# Predict the closest subject for each test vector\n",
    "predictions = [find_closest_subject(test_vector, centroids) for test_vector in test_vectors]\n",
    "\n",
    "# Ground truth labels from the test DataFrame\n",
    "true_labels = sample_subjects_test[column].values\n",
    "\n",
    "# Calculate accuracy, precision, and recall\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "precision = precision_score(true_labels, predictions, average='macro')\n",
    "recall = recall_score(true_labels, predictions, average='macro')\n",
    "\n",
    "print(f\"Predictions: {predictions}\")\n",
    "print(f\"True Labels: {true_labels}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
