{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (C) 2024  Edion Management Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare example file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dataset_dir = \"fine_tune_dataset\"\n",
    "if not os.path.exists(dataset_dir):\n",
    "    os.makedirs(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"conversation_examples.json\", 'r') as file:\n",
    "    conversations = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sending batch api request (Faster and cheaper (50%) than single request) to create synthetic conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the maximum tokens limitation (2,000,000), we devide them into batches, each batch contains 600 requests (3000x600 =1,800,000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = 10 # We want the a dataset size 600x10 = 6,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete progress\n",
    "from openai import OpenAI\n",
    "from prompt import *\n",
    "import time\n",
    "\n",
    "\n",
    "OPENAI_API_KEY = \"sk-proj-ltmkMm6qZ8oQCsusN5IOT3BlbkFJmsPopivPYwLtY7jlx5Pl\"\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "\n",
    "def create_batch_file(index):\n",
    "    \"\"\"\n",
    "    Create batch file that contains json objects, each json object is an api request.\n",
    "    Args:\n",
    "        index: i'th batch\n",
    "    returns:\n",
    "        The saved jsonl file\n",
    "    \"\"\"\n",
    "    batch_file = f'batch_request/batch_synthetic{index}.jsonl'\n",
    "    with open(batch_file, 'w') as f:\n",
    "        count = 0\n",
    "        # Repeat all 6 conversation examples 100 times to have 600 requests\n",
    "        for repeat in range(100):\n",
    "            for i, c in enumerate(conversations):\n",
    "                conversation_str = str(c)\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": f\"{conversation_sys_prompt}\\n{conversation_str}\"}\n",
    "                ]\n",
    "\n",
    "                task = {\n",
    "                    \"custom_id\": f\"synthetic{count}_from_example{i}\",\n",
    "                    \"method\": \"POST\",\n",
    "                    \"url\": \"/v1/chat/completions\",\n",
    "                    \"body\": {\n",
    "                        # Chat Completions API call\n",
    "                        \"model\": \"gpt-4o-mini\",\n",
    "                        \"temperature\": 1,\n",
    "                        \"max_tokens\": 3000,\n",
    "                        \"messages\": messages\n",
    "                    }\n",
    "                } \n",
    "\n",
    "                f.write(json.dumps(task) + '\\n')\n",
    "                count += 1\n",
    "    return batch_file\n",
    "\n",
    "\n",
    "def submit_and_retrival(client, batch_file):\n",
    "    \"\"\"\n",
    "    Submit batch api request and retrival result\n",
    "    Args:\n",
    "        client: OpenAI client\n",
    "        count: i'th batch\n",
    "        batch_file: a jsonl file contains all batch requests\n",
    "    Returns:\n",
    "        Save results in files\n",
    "    \"\"\"\n",
    "    # Upload batch file\n",
    "    batch_input_file = client.files.create(\n",
    "        file=open(batch_file, \"rb\"),\n",
    "        purpose=\"batch\"\n",
    "    )\n",
    "\n",
    "    batch_input_file_id = batch_input_file.id\n",
    "\n",
    "    # Submit batch and start progressing\n",
    "    sent = client.batches.create(\n",
    "        input_file_id=batch_input_file_id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\",\n",
    "        metadata={\n",
    "        \"description\": f\"create synthetic conversations {i}\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    batch_id = sent.to_dict()[\"id\"]\n",
    "    print(f\"Batch: {batch_id} submited\")\n",
    "\n",
    "    # Monitor and retrival results\n",
    "    complete = False\n",
    "    while not complete:\n",
    "        res = client.batches.retrieve(batch_id).to_dict()\n",
    "        status = res[\"status\"]\n",
    "        completed_counts = res[\"request_counts\"][\"completed\"]\n",
    "        if status == \"completed\":\n",
    "            complete = True\n",
    "            output_file_id = res[\"output_file_id\"]\n",
    "            break\n",
    "        time.sleep(30)\n",
    "        print(f\"Batch status: {status}, completed: {completed_counts}\")\n",
    "\n",
    "    print(f\"Batch: {batch_id} completed\")\n",
    "\n",
    "    # Write raw results to file\n",
    "    file_response = client.files.content(output_file_id)\n",
    "    file_response.write_to_file(f\"batch_request/batch_synthetic_output{i}.txt\")\n",
    "\n",
    "    with open(f\"batch_request/batch_synthetic_output{i}.txt\", \"r\") as f:\n",
    "        lines = f.read().splitlines()\n",
    "\n",
    "    # Write the actual synthetic conversations into jsonl file\n",
    "    with open(f\"batch_request/batch_synthetic_conversation{i}.jsonl\", \"w\") as f:\n",
    "        for line in lines:\n",
    "            text = json.loads(line)\n",
    "            result = text[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "            clean_result = result.replace(\"```json\\n\", \"\").replace(\"\\n```\", \"\").replace(\"null\", \"None\")\n",
    "            try:\n",
    "                f.write(json.dumps(eval(clean_result)) + '\\n')\n",
    "            except:\n",
    "                print(clean_result)\n",
    "\n",
    "\n",
    "# Do it in batches\n",
    "for i in range(num_batches):\n",
    "    batch_file = create_batch_file(i)\n",
    "    submit_and_retrival(client, i, batch_file=batch_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all submited batches and check their status\n",
    "client.batches.list().dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all results as dataset in sharGPT format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "\n",
    "conversations = []\n",
    "for file in glob.glob(f'batch_request/batch_synthetic_conversation*.jsonl'):\n",
    "    with open(file, \"r\") as f:\n",
    "        conversations = conversations + [json.loads(line) for line in f.read().splitlines()]\n",
    "\n",
    "with open(\"batch_request/all_synthetic_conversations.jsonl\", \"w\") as f:\n",
    "    [f.write(json.dumps(conv) + '\\n') for conv in conversations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6119"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(conversations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process and clean data (For tool calling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def generate_exercise(subjects: tuple, exercise_types: tuple, grade_levels: tuple, modifications: tuple, ready: bool) -> float:\n",
    "    \"\"\"\n",
    "    Generate excercise based on user preference. \n",
    "    \n",
    "    Args:\n",
    "        subjects: a tuple of exercise subjects for each exercise in order\n",
    "        exercise_types: a tuple of exercise type for each exercise in order\n",
    "        grade_levels: a tuple of grade levels for each exercise in order\n",
    "        modifications: a tuple of modified information for each exercise in order\n",
    "        reday: if the user give all paramters he can and its ready to generated exercise\n",
    "    Returns:\n",
    "        A list of exercises based on user's preference, as a string.\n",
    "    \"\"\"\n",
    "    assert len(subjects) == len(exercise_types) == len(grade_levels) == len(modifications)\n",
    "    if ready:\n",
    "        return (\"exercise 1\", \"exercise 2\", \"exercise 3\")\n",
    "    else:\n",
    "        return \"Not ready yet, ask the user for more information.\"\n",
    "\n",
    "\n",
    "def get_params(ori_params_group: tuple):\n",
    "    \"\"\"\n",
    "    Turn a tuple of parameters (string) into a paramters dict, while modifying parameter names\n",
    "    \"\"\"\n",
    "    new_params = {'subjects': (), 'exercise_types': (),  'grade_levels': (), 'modifications': (), 'ready': False}\n",
    "    for params in ori_params_group:\n",
    "        new_params = {\n",
    "                    'subjects': (*new_params['subjects'], params['S']),\n",
    "                    'exercise_types': (*new_params['exercise_types'], params['T']), \n",
    "                    'grade_levels': (*new_params['grade_levels'], params['G']),\n",
    "                    'modifications': (*new_params['modifications'], params['M']),\n",
    "                    'ready': bool(params['R'])\n",
    "                    }\n",
    "    return new_params\n",
    "\n",
    "\n",
    "def find_params(msg):\n",
    "    \"\"\"\n",
    "    Given a message, find exercise parameters\n",
    "    \"\"\"\n",
    "    params_dict = re.findall(r'<E>(.*?)</E>', msg, re.DOTALL)\n",
    "    if not params_dict:\n",
    "        params_dict = re.findall(r'<E>(.*)', msg, re.DOTALL)\n",
    "    if params_dict:  # Clean data\n",
    "        params_dict[0] = params_dict[0].replace('false', 'False').replace('true', 'True')\n",
    "    return params_dict\n",
    "\n",
    "\n",
    "def process_one_sameple(sample, only_multi_params=False, only_single_params=False):\n",
    "    \"\"\"\n",
    "    Process a conversation sample, turn it into tool calling openai format, and clean data at the same time.\n",
    "    \"\"\"\n",
    "    multi_params_flag = False\n",
    "    new_conversation = []\n",
    "    for dialogue in sample[\"conversations\"]:\n",
    "        new_dict = {}\n",
    "        params_dict = find_params(dialogue['value']) # Check if carying parameters\n",
    "        if dialogue['from'] == \"gpt\":\n",
    "            new_dict['role'] = 'assistant'\n",
    "            old_msg = re.findall(r'<U>(.*?)</U>', dialogue['value'].replace('<\\\\/U>', '</U>'), re.DOTALL) # Check if carying response to user\n",
    "            if old_msg: # When gpt carying response to user normally\n",
    "                old_msg = old_msg[0]\n",
    "                if params_dict: # If gpt response with parameters\n",
    "                    try:\n",
    "                        params = eval(params_dict[0])\n",
    "                    except:\n",
    "                        print(\"ERROR -- Response to user in <E></E>: \", dialogue) # Clean data like: {'from': 'gpt', 'value': '<U>Got it! Here’s your fill-in-the-blank exercise for elementary Spanish vocabulary:</U> <E>1. Yo ______ (to be) a estudiante. 2. Ella ______ (to eat) una pizza. 3. Nosotros ______ (to play) fútbol en el parque.</E>'}\n",
    "                        return [] # exclude dirty conversation\n",
    "                    if type(params) == dict:   # When it comes only one params set\n",
    "                            new_params = get_params((params,))\n",
    "                    elif type(params) == tuple:   # When it is a tuple of params\n",
    "                            print(\"Multiple params *** \", dialogue)\n",
    "                            multi_params_flag = True\n",
    "                            new_params = get_params(params)\n",
    "                    else: \n",
    "                        print('ERROR -- Incorrect params 1: ', params) # Clean data like </E>('S': 'xxx', ...}</E>\n",
    "                        return [] # Only consider dict and tuple, otherwise it would be a dirty dialogue the whole conversation is excluded\n",
    "                    if new_params['ready']:   # When ready\n",
    "                        tool_call = {\"name\": \"generate_exercise\", \"arguments\": {**new_params}}\n",
    "                        new_dict['tool_calls'] = [{\"type\": \"function\", \"function\": tool_call}]\n",
    "                    else:   # Still need more paramters\n",
    "                        new_dict['content'] = old_msg\n",
    "                else:   # When gpt present exercise to user\n",
    "                    new_dict['content'] = old_msg\n",
    "\n",
    "            else: # When 'gpt' accidently carry message <E> exercise </E>, correct it to be 'human'/'tool' (data cleaning)\n",
    "                try:\n",
    "                    tool_result = params_dict[0]\n",
    "                except:\n",
    "                    print(\"ERROR -- Incorrect params 2\", dialogue) # Clean data like: 'E': '{\"S\": \"Language Arts\", \"T\": \"Writing Prompt\", \"G\": None, \"M\": \"Storytelling about animals\", \"R\": False}'}\n",
    "                    return []\n",
    "                new_dict['role'] = 'tool'\n",
    "                new_dict['name'] = 'generate_exercise'\n",
    "                new_dict['content'] = (tool_result,)\n",
    "\n",
    "        elif dialogue['from'] == \"human\":\n",
    "            if params_dict: # If human is expert responding with generated exercise\n",
    "                tool_result = params_dict[0]\n",
    "                new_dict['role'] = 'tool'\n",
    "                new_dict['name'] = 'generate_exercise'\n",
    "                new_dict['content'] = (tool_result,)\n",
    "            else:   # If human response to gpt's answer\n",
    "                new_dict['role'] = 'user'\n",
    "                try:\n",
    "                    new_dict['content'] = re.findall(r'<U>(.*?)</U>', dialogue['value'].replace('<\\\\/U>', '</U>'), re.DOTALL)[0]\n",
    "                except:\n",
    "                    print(\"ERROR -- Message without <U></U>\", dialogue) # Clean data like: {'from': 'human', 'value': \"Yes, let's focus on chemical reactions.\"}\n",
    "                    return []\n",
    "\n",
    "        new_conversation.append(new_dict)\n",
    "    \n",
    "\n",
    "    assert not (only_multi_params and only_single_params), \"Can't be True at the same time\"\n",
    "\n",
    "    if only_multi_params:\n",
    "        if multi_params_flag:\n",
    "            return new_conversation \n",
    "        else:\n",
    "            return []\n",
    "    elif only_single_params:\n",
    "        if multi_params_flag:\n",
    "            return []\n",
    "        else:\n",
    "            return new_conversation \n",
    "    else:\n",
    "        return new_conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Reformat and clean the data\"\"\"\n",
    "import json\n",
    "\n",
    "multi_params_conversations = []\n",
    "for i in range(1, 14):\n",
    "    with open(f\"batch_request/batch_synthetic_conversation{i}.jsonl\", \"r\") as f:\n",
    "        samples = [json.loads(line) for line in f.read().splitlines()]\n",
    "\n",
    "    for sample in samples:\n",
    "        new_conversation = process_one_sameple(sample, only_multi_params=True, only_single_params=False)\n",
    "        if new_conversation:\n",
    "            multi_params_conversations.append({\"messages\": new_conversation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Reformat and clean the data\"\"\"\n",
    "import json\n",
    "with open(\"batch_request/batch_synthetic_conversation13.jsonl\", \"r\") as f:\n",
    "    samples = [json.loads(line) for line in f.read().splitlines()]\n",
    "\n",
    "single_params_conversations = []\n",
    "for sample in samples:\n",
    "    new_conversation = process_one_sameple(sample, only_multi_params=False, only_single_params=True)\n",
    "    if new_conversation:\n",
    "        single_params_conversations.append({\"messages\": new_conversation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "final_conversations = single_params_conversations + multi_params_conversations\n",
    "random.shuffle(final_conversations)\n",
    "\n",
    "with open(\"batch_request/synthetic_conversations13+multi_params.json\", \"w\") as f:\n",
    "    json.dump(final_conversations, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manually clean the dataset\n",
    "1. Search \"two more\", \"three more\", \"some more\", \"two additional\" and check if the number of parameters is correct\n",
    "2. Replace all \"1\" to 1, apply the rule to all numbers from 1-11\n",
    "3. Replace all <U> to \"\"\n",
    "4. Found more based on testing (see what kind of mistakes the model made)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
