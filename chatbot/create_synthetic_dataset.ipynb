{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (C) 2024  Edion Management Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare example file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dataset_dir = \"fine_tune_dataset\"\n",
    "if not os.path.exists(dataset_dir):\n",
    "    os.makedirs(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"conversation_examples.json\", 'r') as file:\n",
    "    conversations = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sending batch api request (Faster and cheaper (50%) than single request) to create synthetic conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the maximum tokens limitation (2,000,000), we devide them into batches, each batch contains 600 requests (3000x600 =1,800,000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = 10 # We want the a dataset size 600x10 = 6,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete progress\n",
    "from openai import OpenAI\n",
    "from prompt import *\n",
    "import time\n",
    "\n",
    "\n",
    "OPENAI_API_KEY = \"sk-proj-ltmkMm6qZ8oQCsusN5IOT3BlbkFJmsPopivPYwLtY7jlx5Pl\"\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "\n",
    "def create_batch_file(index):\n",
    "    \"\"\"\n",
    "    Create batch file that contains json objects, each json object is an api request.\n",
    "    Args:\n",
    "        index: i'th batch\n",
    "    returns:\n",
    "        The saved jsonl file\n",
    "    \"\"\"\n",
    "    batch_file = f'batch_request/batch_synthetic{index}.jsonl'\n",
    "    with open(batch_file, 'w') as f:\n",
    "        count = 0\n",
    "        # Repeat all 6 conversation examples 100 times to have 600 requests\n",
    "        for repeat in range(100):\n",
    "            for i, c in enumerate(conversations):\n",
    "                conversation_str = str(c)\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": f\"{conversation_sys_prompt}\\n{conversation_str}\"}\n",
    "                ]\n",
    "\n",
    "                task = {\n",
    "                    \"custom_id\": f\"synthetic{count}_from_example{i}\",\n",
    "                    \"method\": \"POST\",\n",
    "                    \"url\": \"/v1/chat/completions\",\n",
    "                    \"body\": {\n",
    "                        # Chat Completions API call\n",
    "                        \"model\": \"gpt-4o-mini\",\n",
    "                        \"temperature\": 1,\n",
    "                        \"max_tokens\": 3000,\n",
    "                        \"messages\": messages\n",
    "                    }\n",
    "                } \n",
    "\n",
    "                f.write(json.dumps(task) + '\\n')\n",
    "                count += 1\n",
    "    return batch_file\n",
    "\n",
    "\n",
    "def submit_and_retrival(client, batch_file):\n",
    "    \"\"\"\n",
    "    Submit batch api request and retrival result\n",
    "    Args:\n",
    "        client: OpenAI client\n",
    "        count: i'th batch\n",
    "        batch_file: a jsonl file contains all batch requests\n",
    "    Returns:\n",
    "        Save results in files\n",
    "    \"\"\"\n",
    "    # Upload batch file\n",
    "    batch_input_file = client.files.create(\n",
    "        file=open(batch_file, \"rb\"),\n",
    "        purpose=\"batch\"\n",
    "    )\n",
    "\n",
    "    batch_input_file_id = batch_input_file.id\n",
    "\n",
    "    # Submit batch and start progressing\n",
    "    sent = client.batches.create(\n",
    "        input_file_id=batch_input_file_id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\",\n",
    "        metadata={\n",
    "        \"description\": f\"create synthetic conversations {i}\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    batch_id = sent.to_dict()[\"id\"]\n",
    "    print(f\"Batch: {batch_id} submited\")\n",
    "\n",
    "    # Monitor and retrival results\n",
    "    complete = False\n",
    "    while not complete:\n",
    "        res = client.batches.retrieve(batch_id).to_dict()\n",
    "        status = res[\"status\"]\n",
    "        completed_counts = res[\"request_counts\"][\"completed\"]\n",
    "        if status == \"completed\":\n",
    "            complete = True\n",
    "            output_file_id = res[\"output_file_id\"]\n",
    "            break\n",
    "        time.sleep(30)\n",
    "        print(f\"Batch status: {status}, completed: {completed_counts}\")\n",
    "\n",
    "    print(f\"Batch: {batch_id} completed\")\n",
    "\n",
    "    # Write raw results to file\n",
    "    file_response = client.files.content(output_file_id)\n",
    "    file_response.write_to_file(f\"batch_request/batch_synthetic_output{i}.txt\")\n",
    "\n",
    "    with open(f\"batch_request/batch_synthetic_output{i}.txt\", \"r\") as f:\n",
    "        lines = f.read().splitlines()\n",
    "\n",
    "    # Write the actual synthetic conversations into jsonl file\n",
    "    with open(f\"batch_request/batch_synthetic_conversation{i}.jsonl\", \"w\") as f:\n",
    "        for line in lines:\n",
    "            text = json.loads(line)\n",
    "            result = text[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "            clean_result = result.replace(\"```json\\n\", \"\").replace(\"\\n```\", \"\").replace(\"null\", \"None\")\n",
    "            try:\n",
    "                f.write(json.dumps(eval(clean_result)) + '\\n')\n",
    "            except:\n",
    "                print(clean_result)\n",
    "\n",
    "\n",
    "# Do it in batches\n",
    "for i in range(num_batches):\n",
    "    batch_file = create_batch_file(i)\n",
    "    submit_and_retrival(client, i, batch_file=batch_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all submited batches and check their status\n",
    "client.batches.list().dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all results as dataset in sharGPT format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "\n",
    "conversations = []\n",
    "for file in glob.glob(f'batch_request/batch_synthetic_conversation*.jsonl'):\n",
    "    with open(file, \"r\") as f:\n",
    "        conversations = conversations + [json.loads(line) for line in f.read().splitlines()]\n",
    "\n",
    "with open(\"batch_request/all_synthetic_conversations.jsonl\", \"w\") as f:\n",
    "    [f.write(json.dumps(conv) + '\\n') for conv in conversations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6119"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(conversations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
