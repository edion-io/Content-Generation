# Copyright (C) 2024  Edion Management Systems
import torch
from transformers import pipeline
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from definition import conversation_sys_prompt


def merge_run(base_model: str, new_model: str, conversation_sys_prompt: str, max_new_tokens: int, temperature: float):
    """
    If only the fine tuned adapter existed (either locally or in huggingface)
    Args:
        base_model (str): The base model or the pretrained model used for fine-tuning, can be a local diretory or huggingface model repos.
        new_model (str): The fine-tuned adapter, can be a local diretory containing .bin file or a remote huggingface repos.
        conversation_sys_prompt (int): System prompt.
        max_new_tokens (float): The max generated tokens generated by LLM each round.
    """

    # Reload base model and its tokenizer
    tokenizer = AutoTokenizer.from_pretrained(base_model)
    tokenizer.pad_token = tokenizer.eos_token

    base_model_reload = AutoModelForCausalLM.from_pretrained(
            base_model,
            return_dict=True,
            low_cpu_mem_usage=True,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True,
    )

    # Merge (fine tuned) adapter with base model
    model = PeftModel.from_pretrained(base_model_reload, new_model)
    model = model.merge_and_unload()

    # Set pipeline for LLM task
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        torch_dtype=torch.float16,
        device_map="auto",
    )

    # Set initial message
    # messages = [{"role": "user", "content": "Translate I love you into Dutch"}]
    # messages = [{"role": "user", "content": "<U> Hi, I need some Math excercise </U>"}]
    messages = [{"role": "system", "content": conversation_sys_prompt},
                {"role": "user", "content": "<U> Hi, I need some Math excercise </U>"}]

    # Generate response
    outputs = pipe(messages, max_new_tokens=max_new_tokens, temperature=temperature)
    response = outputs[0]["generated_text"][-1]["content"]
    print(response)
    messages.append({"role": "assistant", "content": response})
    print("-------------------------------")

    # Conversation loop
    while True:
        user_input = input("Input: ")
        messages.append({'role': 'user', 'content': user_input})
        outputs = pipe(messages, max_new_tokens=256, temperature=0.7)
        response = outputs[0]["generated_text"][-1]["content"]
        print(response)
        messages.append({"role": "assistant", "content": response})
        print("-------------------------------")


def run(model: str, max_new_tokens: int, temperature: float):
    """
    If an merged model already exists in huggingface
    Args:
        model (str): The complete merged fine-tuned model, can be a local diretory or huggingface model repos.
        conversation_sys_prompt (int): System prompt.
        max_new_tokens (float): The max generated tokens generated by LLM each round.
    """
    # Set pipeline for LLM task
    pipe = pipeline(
        "text-generation",
        model=model,
        torch_dtype=torch.float16,
        device_map="auto",
    )

    # Set initial message
    messages = [{"role": "system", "content": conversation_sys_prompt},
                {"role": "user", "content": "<U> Hi </U>"}]

    # Generate response
    outputs = pipe(messages, max_new_tokens=max_new_tokens, temperature=temperature)
    response = outputs[0]["generated_text"][-1]["content"]
    print(response)
    messages.append({"role": "assistant", "content": response})
    print("-------------------------------")

    # Conversation loop
    while True:
        user_input = input("Input: ")
        messages.append({'role': 'user', 'content': user_input})
        outputs = pipe(messages, max_new_tokens=max_new_tokens, temperature=temperature)
        response = outputs[0]["generated_text"][-1]["content"]
        print(response)
        messages.append({"role": "assistant", "content": response})
        print("-------------------------------")



if __name__ == "__main__":

    final_fine_tuned_model_exists = True
    max_new_tokens = 256
    temperature = 0.7

    if final_fine_tuned_model_exists:
        base_model = "meta-llama/Meta-Llama-3.1-8B-Instruct"
        new_model = "llama3.1-8b-chat-exercise/checkpoint-3059"
        merge_run(base_model, new_model, max_new_tokens, temperature)

    else:
        final_model = "kangsive/llama3.1-8b-chat-exercise"
        run(final_model, max_new_tokens, temperature)